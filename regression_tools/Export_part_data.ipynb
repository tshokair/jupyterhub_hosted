{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from urllib import parse\n",
    "from lib.build_export_features import MixedFeatureData\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "FOLLOWER_URL = os.environ['FOLLOWER_URL']\n",
    "                          \n",
    "def make_connection():\n",
    "    url_output = FOLLOWER_URL\n",
    "    url = urlparse(url_output)\n",
    "    conn = psycopg2.connect(\n",
    "        database=url.path[1:],\n",
    "        user=url.username,\n",
    "        password=url.password,\n",
    "        host=url.hostname\n",
    "    )\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START HERE\n",
    "## The cell below contains all the mission specific data. Here you will specifity the mission id, the independent variables, the dependent variable, and the definitions of positive and negative outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THESE VALUES\n",
    "mission_id = \"25614\"\n",
    "# ONLY USE SINGLE QUESTIONS FOR DEPENDENT AND INDEPENDENT VARIABLES IN FORMAT 'PART_NUM-QUESTION_NUM'\n",
    "# ALL \"4-4\", \"4-5\",\"4-6\", \"4-10\",\"4-11\",\"4-12\", \"4-13\",\"4-14\",\"4-15\",\"4-17\",\"4-18\",\"4-19\",\"4-21\"\n",
    "continuous_independent_variables = [\n",
    "    \"4-4\", \"4-5\",\"4-6\", \"4-10\",\"4-11\",\"4-12\", \"4-13\",\"4-14\",\"4-15\",\"4-17\",\"4-18\",\"4-19\",\"4-21\"\n",
    "]\n",
    "# NOTE MULTIPLE QUESTIONS NEED TO BE CATEGORICAL\n",
    "categorical_independent_variables = []\n",
    "binary_independent_variables = []\n",
    "dependent_variable = \"4-2\"\n",
    "negative_outcomes = [\n",
    "\"Moderately satisfied\",\n",
    "\"Slightly satisfied\",\n",
    "\"Neither satisfied nor dissatisfied\",\n",
    "\"Slightly dissatisfied\",\n",
    "\"Moderately dissatisfied\",\n",
    "\"Extremely dissatisfied\"\n",
    "]\n",
    "positive_outcomes = [\"Extremely satisfied\"]\n",
    "demo_independent_variables = []\n",
    "tag_independent_variables = []\n",
    "#mutually exclusive scout groups only at this time\n",
    "scout_group_independent_variables = []\n",
    "#response_data = pd.read_pickle(\"mission_\" + mission_id + \"_data.pkl\")\n",
    "question_response_filtering = {}\n",
    "grouping = 'user_id'\n",
    "ethnicity_filters = []\n",
    "education_filters = []\n",
    "tag_filters = []\n",
    "scout_group_filters = [\"Pixel\",\"Apple\",\"Samsung\",\"Bose\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After entering the information above, go to Cell -> Run All to see your regression results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collected questions\n",
      "collected snippets\n",
      "collected scout groups\n",
      "collected responses\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function connection.close>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnc = make_connection()\n",
    "questions = pd.read_sql_query(\n",
    "    \"\"\"with m_questions as (select id as question_id, label, type, position as question_position,\n",
    "                            part_id, structure from questions where type in \n",
    "                            ('SingleQuestion', 'MultipleQuestion', 'NumberQuestion')\n",
    "                            and mission_id = \"\"\"\n",
    "    + mission_id\n",
    "    + \"\"\"),\n",
    "       m_parts as (select id as part_id, position as part_position from parts where\n",
    "                   mission_id = \"\"\"\n",
    "    + mission_id\n",
    "    + \"\"\")\n",
    "    select question_id, label, type, question_position, m_parts.part_id, part_position,\n",
    "    structure from m_questions join m_parts on m_questions.part_id = m_parts.part_id\"\"\",\n",
    "    cnc,\n",
    ")\n",
    "print(\"collected questions\")\n",
    "\n",
    "question_list = (\n",
    "    \"(\" + \",\".join([str(a) for a in questions[\"question_id\"].tolist()]) + \")\"\n",
    ")\n",
    "snippets = pd.read_sql_query(\n",
    "    \"\"\"select id as snippet_id, user_id, assignment_id from snippets where mission_id = \"\"\"\n",
    "    + mission_id,\n",
    "    cnc,\n",
    ")\n",
    "print(\"collected snippets\")\n",
    "scout_group_names = (\n",
    "    \"(\"\n",
    "    + \",\".join([\"'\" + str(sg) + \"'\" for sg in scout_group_independent_variables])\n",
    "    + \")\"\n",
    ")\n",
    "assignments = pd.read_sql_query(\n",
    "    \"\"\"select id as assignment_id, user_id from assignments where \n",
    "          mission_id = \"\"\"\n",
    "    + mission_id\n",
    "    ,cnc)\n",
    "assignment_list = \"(\" + ','.join([str(aid) for aid in assignments['assignment_id'].unique()]) +\")\"\n",
    "assignment_groups = pd.read_sql_query(\n",
    "    \"\"\"select assignment_id, scout_group_id from assignments_scout_groups\n",
    "    where assignment_id in \"\"\" + assignment_list,\n",
    "    cnc\n",
    ")\n",
    "groups = pd.read_sql_query(\n",
    "    \"\"\"select id as scout_group_id, name as scout_group from scout_groups\n",
    "    \"\"\",\n",
    "    cnc)\n",
    "scout_groups = pd.merge(\n",
    "    assignment_groups,\n",
    "    groups[groups['scout_group'].isin(scout_group_filters)],\n",
    "    on='scout_group_id'\n",
    ")\n",
    "print(\"collected scout groups\")\n",
    "\n",
    "\n",
    "snippet_list = (\n",
    "    \"(\" + \",\".join([str(a) for a in snippets[\"snippet_id\"].tolist()]) + \")\"\n",
    ")\n",
    "responses = pd.read_sql_query(\n",
    "    \"\"\"select  id as response_id, snippet_id, question_id, \n",
    "    answers from responses where snippet_id in \"\"\"\n",
    "    + snippet_list,\n",
    "    cnc,\n",
    ")\n",
    "print(\"collected responses\")\n",
    "snippets_tags = pd.read_sql_query(\n",
    "    \"\"\"Select tag_id, snippet_id from snippets_tags where snippet_id in \"\"\"\n",
    "    + snippet_list,\n",
    "    cnc,\n",
    ")\n",
    "tag_list = \"(\" + \",\".join([str(a) for a in snippets_tags[\"tag_id\"].tolist()]) + \")\"\n",
    "tags = pd.read_sql_query(\n",
    "    \"\"\"select id as tag_id, name as tag from tags where id in \"\"\" + tag_list, cnc\n",
    ")\n",
    "\n",
    "snippets_tags = pd.merge(snippets_tags, tags, on=\"tag_id\")\n",
    "tagged_snippets = pd.merge(snippets, snippets_tags, on=\"snippet_id\", how=\"left\")\n",
    "user_list = \"(\" + \",\".join([str(a) for a in assignments[\"user_id\"].tolist()]) + \")\"\n",
    "users = pd.read_sql_query(\n",
    "    \"\"\"select id as user_id, ethnicity, education, household_income, birthday, gender from users where\n",
    "                                 id in \"\"\"\n",
    "    + user_list,\n",
    "    cnc,\n",
    ")\n",
    "user_snippets = pd.merge(tagged_snippets, users, on=\"user_id\", how=\"left\")\n",
    "user_snippets = pd.merge(user_snippets, scout_groups, on=\"assignment_id\", how=\"left\")\n",
    "response_data = pd.merge(responses, questions, on=\"question_id\")\n",
    "response_data = pd.merge(response_data, user_snippets, on=\"snippet_id\", how=\"left\")\n",
    "cnc.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_data['birthday'] = response_data['birthday'].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing\n"
     ]
    }
   ],
   "source": [
    "fd = MixedFeatureData(\n",
    "    response_data,\n",
    "    dependent_variable,\n",
    "    continuous_independent_variables,\n",
    "    binary_independent_variables,\n",
    "    categorical_independent_variables,\n",
    "    positive_outcomes,\n",
    "    negative_outcomes,\n",
    "    demo_independent_variables,\n",
    "    tag_independent_variables,\n",
    "    scout_group_independent_variables,\n",
    "    grouping,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = [p for p in response_data['part_position'].unique()]\n",
    "#parts\n",
    "part_questions = [response_data[response_data['part_position'] == p]['question_position'].unique() for p in parts]\n",
    "part_questions\n",
    "question_names = [[str(1+p)+'-'+str(1+q) for q in part_questions[p]] for p in parts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unlistify(df, column):\n",
    "    matches = [i for i,n in enumerate(df.columns)\n",
    "             if n==column]\n",
    "\n",
    "    if len(matches)==0:\n",
    "        raise Exception('Failed to find column named ' + column +'!')\n",
    "    if len(matches)>1:\n",
    "        raise Exception('More than one column named ' + column +'!')\n",
    "\n",
    "    col_idx = matches[0]\n",
    "\n",
    "  # Helper function to expand and repeat the column col_idx\n",
    "    def fnc(d):\n",
    "        row = list(d.values[0])\n",
    "        bef = row[:col_idx]\n",
    "        aft = row[col_idx+1:]\n",
    "        col = row[col_idx]\n",
    "        z = [bef + [c] + aft for c in col]\n",
    "        return pd.DataFrame(z)\n",
    "\n",
    "    col_idx += len(df.index.shape) # Since we will push reset the index\n",
    "    index_names = list(df.index.names)\n",
    "    column_names = list(index_names) + list(df.columns)\n",
    "    return (df\n",
    "          .reset_index()\n",
    "          .groupby(level=0,as_index=0)\n",
    "          .apply(fnc)\n",
    "          .rename(columns = lambda i :column_names[i])\n",
    "          .set_index(index_names)\n",
    "          )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = fd.processed_df#[fd.processed_df['type'] != 'MultipleQuestion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parts:\n",
    "    \n",
    "    part_df_single = (\n",
    "        processed[\n",
    "            (processed['question_name'].isin(question_names[p])) &\n",
    "            (processed['type'] != 'MultipleQuetsion') \n",
    "        ]\n",
    "        [['snippet_id','question_name','response_clean']]\n",
    "        .pivot(\n",
    "            index='snippet_id', columns=\"question_name\", values=\"response_clean\"\n",
    "        ).reset_index()\n",
    "    )\n",
    "    part_df_multiple = pd.DataFrame()\n",
    "    for qn in question_names[p]:\n",
    "        if processed[processed['question_name'] == qn]['type'].iloc[0] != 'MultipleQuestion':\n",
    "            pass\n",
    "        else:\n",
    "            q_df = processed[processed['question_name'] == qn][['snippet_id','answers']]\n",
    "            q_df = unlistify(q_df,'answers')\n",
    "            q_df = pd.get_dummies(q_df[['snippet_id','answers']], columns=['answers'], prefix=qn)\n",
    "            if len(part_df_multiple) == 0:\n",
    "                part_df_multiple = q_df.copy()\n",
    "            else:\n",
    "                part_df_multiple = pd.merge(part_df_multiple, q_df, on='snippet_id')\n",
    "    if len(part_df_multiple) == 0:\n",
    "        part_df = part_df_single.copy()\n",
    "    else:\n",
    "        part_df = pd.merge(\n",
    "            part_df_single,\n",
    "            part_df_multiple,\n",
    "            on='snippet_id'\n",
    "        )\n",
    "        \n",
    "    part_df = pd.merge(\n",
    "        part_df,\n",
    "        user_snippets,\n",
    "        on='snippet_id'\n",
    "    )\n",
    "    part_df.to_csv('mission_'+mission_id+'_part_'+str(p)+'_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
